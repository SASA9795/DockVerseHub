# 09_advanced_tricks/custom-solutions/sidecar-patterns.yml

# Sidecar Container Patterns
# Advanced sidecar implementations for logging, monitoring, security, and service mesh

apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-with-sidecars
  namespace: default
  labels:
    app: webapp-sidecars
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp-sidecars
  template:
    metadata:
      labels:
        app: webapp-sidecars
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      containers:
        # Main application container
        - name: webapp
          image: nginx:alpine
          ports:
            - containerPort: 80
              name: http
          volumeMounts:
            - name: app-logs
              mountPath: /var/log/nginx
            - name: shared-data
              mountPath: /usr/share/nginx/html
            - name: nginx-config
              mountPath: /etc/nginx/conf.d
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          livenessProbe:
            httpGet:
              path: /
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 10

        # Logging sidecar - Fluent Bit
        - name: log-collector
          image: fluent/fluent-bit:2.1
          volumeMounts:
            - name: app-logs
              mountPath: /var/log/nginx
            - name: fluentbit-config
              mountPath: /fluent-bit/etc
          env:
            - name: FLUENTD_HOST
              value: "fluentd-service"
            - name: FLUENTD_PORT
              value: "24224"
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Monitoring sidecar - Prometheus Node Exporter
        - name: metrics-exporter
          image: prom/node-exporter:latest
          ports:
            - containerPort: 9100
              name: metrics
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --collector.filesystem.ignored-mount-points
            - ^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly: true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: rootfs
              mountPath: /rootfs
              readOnly: true
          resources:
            requests:
              memory: "32Mi"
              cpu: "25m"
            limits:
              memory: "64Mi"
              cpu: "50m"

        # Security sidecar - OWASP ModSecurity
        - name: security-proxy
          image: owasp/modsecurity:latest
          ports:
            - containerPort: 8080
              name: security
          env:
            - name: BACKEND
              value: "http://localhost:80"
            - name: MODSECURITY_CONF
              value: "/etc/modsecurity/modsecurity.conf"
          volumeMounts:
            - name: modsecurity-config
              mountPath: /etc/modsecurity
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"

        # Service mesh sidecar - Envoy Proxy
        - name: envoy-proxy
          image: envoyproxy/envoy:v1.27-latest
          ports:
            - containerPort: 9901
              name: admin
            - containerPort: 15000
              name: proxy
          command:
            - /usr/local/bin/envoy
          args:
            - -c
            - /etc/envoy/envoy.yaml
            - --service-cluster
            - webapp-cluster
            - --service-node
            - webapp-node
          volumeMounts:
            - name: envoy-config
              mountPath: /etc/envoy
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Configuration hot-reload sidecar
        - name: config-watcher
          image: alpine:latest
          command: ["/bin/sh"]
          args:
            - -c
            - |
              apk add --no-cache inotify-tools curl
              while true; do
                inotifywait -e modify /etc/config/
                echo "Configuration changed, reloading..."
                curl -X POST http://localhost:80/reload || true
                sleep 5
              done
          volumeMounts:
            - name: dynamic-config
              mountPath: /etc/config
          resources:
            requests:
              memory: "16Mi"
              cpu: "10m"
            limits:
              memory: "32Mi"
              cpu: "20m"

      volumes:
        - name: app-logs
          emptyDir: {}
        - name: shared-data
          emptyDir: {}
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: rootfs
          hostPath:
            path: /
        - name: nginx-config
          configMap:
            name: nginx-config
        - name: fluentbit-config
          configMap:
            name: fluentbit-config
        - name: modsecurity-config
          configMap:
            name: modsecurity-config
        - name: envoy-config
          configMap:
            name: envoy-config
        - name: dynamic-config
          configMap:
            name: dynamic-config

---
# Advanced Sidecar Pattern - Ambassador
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database-with-ambassador
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: database-ambassador
  template:
    metadata:
      labels:
        app: database-ambassador
    spec:
      containers:
        # Main database container
        - name: postgres
          image: postgres:15-alpine
          env:
            - name: POSTGRES_DB
              value: "myapp"
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: username
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
          ports:
            - containerPort: 5432
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"

        # Ambassador sidecar for connection pooling
        - name: pgbouncer
          image: pgbouncer/pgbouncer:latest
          ports:
            - containerPort: 6432
              name: pgbouncer
          env:
            - name: DATABASES_HOST
              value: "localhost"
            - name: DATABASES_PORT
              value: "5432"
            - name: DATABASES_USER
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: username
            - name: DATABASES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
            - name: DATABASES_DBNAME
              value: "myapp"
            - name: POOL_MODE
              value: "transaction"
            - name: MAX_CLIENT_CONN
              value: "100"
            - name: DEFAULT_POOL_SIZE
              value: "25"
          volumeMounts:
            - name: pgbouncer-config
              mountPath: /etc/pgbouncer
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Database monitoring sidecar
        - name: postgres-exporter
          image: prometheuscommunity/postgres-exporter:latest
          ports:
            - containerPort: 9187
              name: metrics
          env:
            - name: DATA_SOURCE_NAME
              value: "postgresql://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@localhost:5432/myapp?sslmode=disable"
          envFrom:
            - secretRef:
                name: postgres-secret
          resources:
            requests:
              memory: "32Mi"
              cpu: "25m"
            limits:
              memory: "64Mi"
              cpu: "50m"

      volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: postgres-pvc
        - name: pgbouncer-config
          configMap:
            name: pgbouncer-config

---
# Adapter Pattern Sidecar
apiVersion: apps/v1
kind: Deployment
metadata:
  name: legacy-app-with-adapter
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: legacy-adapter
  template:
    metadata:
      labels:
        app: legacy-adapter
    spec:
      containers:
        # Legacy application (outputs to file)
        - name: legacy-app
          image: legacy-app:latest
          volumeMounts:
            - name: app-output
              mountPath: /app/output
            - name: app-logs
              mountPath: /var/log/app
          env:
            - name: OUTPUT_DIR
              value: "/app/output"
            - name: LOG_DIR
              value: "/var/log/app"
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"

        # Adapter sidecar (converts file output to HTTP API)
        - name: file-to-http-adapter
          image: alpine:latest
          ports:
            - containerPort: 8080
              name: http-api
          command: ["/bin/sh"]
          args:
            - -c
            - |
              apk add --no-cache python3 py3-pip inotify-tools
              pip3 install flask watchdog

              cat > /app/adapter.py << 'EOF'
              from flask import Flask, jsonify
              import json
              import os
              from watchdog.observers import Observer
              from watchdog.events import FileSystemEventHandler

              app = Flask(__name__)
              latest_data = {}

              class FileHandler(FileSystemEventHandler):
                  def on_modified(self, event):
                      if event.src_path.endswith('.json'):
                          try:
                              with open(event.src_path, 'r') as f:
                                  global latest_data
                                  latest_data = json.load(f)
                          except Exception as e:
                              print(f"Error reading file: {e}")

              @app.route('/api/data')
              def get_data():
                  return jsonify(latest_data)

              @app.route('/health')
              def health():
                  return jsonify({"status": "healthy"})

              if __name__ == '__main__':
                  observer = Observer()
                  observer.schedule(FileHandler(), '/app/output', recursive=True)
                  observer.start()
                  app.run(host='0.0.0.0', port=8080)
              EOF

              python3 /app/adapter.py
          volumeMounts:
            - name: app-output
              mountPath: /app/output
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Log adapter sidecar (converts custom log format to JSON)
        - name: log-adapter
          image: alpine:latest
          command: ["/bin/sh"]
          args:
            - -c
            - |
              apk add --no-cache python3

              cat > /app/log_adapter.py << 'EOF'
              import re
              import json
              import sys
              from datetime import datetime

              # Custom log format: [TIMESTAMP] LEVEL: MESSAGE
              log_pattern = re.compile(r'\[([^\]]+)\]\s+(\w+):\s+(.*)')

              def convert_log_line(line):
                  match = log_pattern.match(line)
                  if match:
                      timestamp, level, message = match.groups()
                      return json.dumps({
                          "timestamp": timestamp,
                          "level": level,
                          "message": message,
                          "service": "legacy-app"
                      })
                  return line

              # Watch and convert logs
              import subprocess
              import select

              proc = subprocess.Popen(['tail', '-f', '/var/log/app/app.log'], 
                                    stdout=subprocess.PIPE, 
                                    stderr=subprocess.PIPE,
                                    universal_newlines=True)

              while True:
                  output = proc.stdout.readline()
                  if output:
                      converted = convert_log_line(output.strip())
                      print(converted)
                      sys.stdout.flush()
              EOF

              python3 /app/log_adapter.py
          volumeMounts:
            - name: app-logs
              mountPath: /var/log/app
          resources:
            requests:
              memory: "32Mi"
              cpu: "25m"
            limits:
              memory: "64Mi"
              cpu: "50m"

      volumes:
        - name: app-output
          emptyDir: {}
        - name: app-logs
          emptyDir: {}

---
# Istio Service Mesh Sidecar Pattern
apiVersion: apps/v1
kind: Deployment
metadata:
  name: microservice-with-istio
  namespace: default
  labels:
    app: microservice
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: microservice
      version: v1
  template:
    metadata:
      labels:
        app: microservice
        version: v1
      annotations:
        sidecar.istio.io/inject: "true"
        sidecar.istio.io/proxyCPU: "100m"
        sidecar.istio.io/proxyMemory: "128Mi"
        sidecar.istio.io/includeInboundPorts: "8080,9090"
        sidecar.istio.io/excludeInboundPorts: "9091"
    spec:
      containers:
        # Main microservice
        - name: microservice
          image: microservice:v1
          ports:
            - containerPort: 8080
              name: http
            - containerPort: 9090
              name: grpc
            - containerPort: 9091
              name: health
          env:
            - name: SERVICE_VERSION
              value: "v1"
            - name: JAEGER_ENDPOINT
              value: "http://jaeger-collector:14268/api/traces"
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          livenessProbe:
            httpGet:
              path: /health
              port: 9091
            initialDelaySeconds: 15
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 9091
            initialDelaySeconds: 5
            periodSeconds: 5

        # Custom metrics sidecar
        - name: metrics-collector
          image: prom/prometheus:latest
          ports:
            - containerPort: 9092
              name: prom-metrics
          command: ["/bin/sh"]
          args:
            - -c
            - |
              cat > /etc/prometheus/prometheus.yml << 'EOF'
              global:
                scrape_interval: 15s
              scrape_configs:
              - job_name: 'microservice'
                static_configs:
                - targets: ['localhost:9090']
              - job_name: 'istio-proxy'
                static_configs:
                - targets: ['localhost:15090']
              EOF
              prometheus --config.file=/etc/prometheus/prometheus.yml \
                         --storage.tsdb.path=/prometheus \
                         --web.console.libraries=/etc/prometheus/console_libraries \
                         --web.console.templates=/etc/prometheus/consoles \
                         --web.enable-lifecycle \
                         --web.listen-address=:9092
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"

---
# ConfigMap for Fluent Bit
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentbit-config
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf

    [INPUT]
        Name              tail
        Path              /var/log/nginx/*.log
        Parser            nginx
        Tag               nginx.*
        Refresh_Interval  5
        Mem_Buf_Limit     5MB

    [OUTPUT]
        Name  forward
        Match *
        Host  ${FLUENTD_HOST}
        Port  ${FLUENTD_PORT}

  parsers.conf: |
    [PARSER]
        Name        nginx
        Format      regex
        Regex       ^(?<remote>[^ ]*) (?<host>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key    time
        Time_Format %d/%b/%Y:%H:%M:%S %z

---
# ConfigMap for Envoy Proxy
apiVersion: v1
kind: ConfigMap
metadata:
  name: envoy-config
data:
  envoy.yaml: |
    admin:
      access_log_path: /tmp/admin_access.log
      address:
        socket_address: { address: 0.0.0.0, port_value: 9901 }

    static_resources:
      listeners:
      - name: listener_0
        address:
          socket_address: { address: 0.0.0.0, port_value: 15000 }
        filter_chains:
        - filters:
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              stat_prefix: ingress_http
              route_config:
                name: local_route
                virtual_hosts:
                - name: local_service
                  domains: ["*"]
                  routes:
                  - match: { prefix: "/" }
                    route: { cluster: webapp_cluster }
              http_filters:
              - name: envoy.filters.http.router
      
      clusters:
      - name: webapp_cluster
        connect_timeout: 0.25s
        type: STATIC
        lb_policy: ROUND_ROBIN
        load_assignment:
          cluster_name: webapp_cluster
          endpoints:
          - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: 127.0.0.1
                    port_value: 80

---
# ConfigMap for PgBouncer
apiVersion: v1
kind: ConfigMap
metadata:
  name: pgbouncer-config
data:
  pgbouncer.ini: |
    [databases]
    myapp = host=127.0.0.1 port=5432 dbname=myapp

    [pgbouncer]
    listen_addr = 0.0.0.0
    listen_port = 6432
    unix_socket_dir =
    user = pgbouncer
    auth_file = /etc/pgbouncer/userlist.txt
    auth_type = trust
    pool_mode = transaction
    server_reset_query = DISCARD ALL
    max_client_conn = 100
    default_pool_size = 25
    log_connections = 1
    log_disconnections = 1
    log_pooler_errors = 1

  userlist.txt: |
    "pgbouncer" ""

---
# ConfigMap for ModSecurity
apiVersion: v1
kind: ConfigMap
metadata:
  name: modsecurity-config
data:
  modsecurity.conf: |
    SecRuleEngine On
    SecRequestBodyAccess On
    SecResponseBodyAccess On
    SecRequestBodyLimit 13107200
    SecRequestBodyNoFilesLimit 131072
    SecRequestBodyInMemoryLimit 131072
    SecRequestBodyLimitAction Reject
    SecAuditEngine RelevantOnly
    SecAuditLogParts ABIJDEFHZ
    SecAuditLog /var/log/modsec_audit.log

    # OWASP Core Rule Set
    Include /etc/modsecurity/crs-setup.conf
    Include /etc/modsecurity/rules/*.conf
