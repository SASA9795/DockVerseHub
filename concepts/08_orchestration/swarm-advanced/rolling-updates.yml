# 08_orchestration/swarm-advanced/rolling-updates.yml

version: '3.8'

services:
  # Blue-Green deployment example
  web-blue:
    image: nginx:1.20-alpine
    networks:
      - app-network
    configs:
      - source: nginx-blue-config
        target: /etc/nginx/nginx.conf
    environment:
      - DEPLOYMENT_COLOR=blue
      - VERSION=1.0.0
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 120s
        max_failure_ratio: 0.1
        order: start-first
      rollback_config:
        parallelism: 1
        delay: 10s
        failure_action: pause
        monitor: 60s
        max_failure_ratio: 0.1
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.deployment != green
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          cpus: '0.5'
          memory: 128M
        reservations:
          cpus: '0.25'
          memory: 64M
      labels:
        - "deployment.color=blue"
        - "deployment.version=1.0.0"
        - "traefik.enable=true"
        - "traefik.http.routers.web-blue.rule=Host(`app.local`) && Headers(`X-Deployment`, `blue`)"
        - "traefik.http.services.web-blue.loadbalancer.server.port=80"
        - "traefik.http.services.web-blue.loadbalancer.healthcheck.path=/health"
        - "traefik.http.services.web-blue.loadbalancer.healthcheck.interval=10s"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Green deployment for blue-green strategy
  web-green:
    image: nginx:1.21-alpine
    networks:
      - app-network
    configs:
      - source: nginx-green-config
        target: /etc/nginx/nginx.conf
    environment:
      - DEPLOYMENT_COLOR=green
      - VERSION=2.0.0
    deploy:
      replicas: 0  # Start with 0, scale up during deployment
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 120s
        max_failure_ratio: 0.1
        order: start-first
      rollback_config:
        parallelism: 1
        delay: 10s
        failure_action: pause
        monitor: 60s
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.deployment != blue
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          cpus: '0.5'
          memory: 128M
        reservations:
          cpus: '0.25'
          memory: 64M
      labels:
        - "deployment.color=green"
        - "deployment.version=2.0.0"
        - "traefik.enable=true"
        - "traefik.http.routers.web-green.rule=Host(`app.local`) && Headers(`X-Deployment`, `green`)"
        - "traefik.http.services.web-green.loadbalancer.server.port=80"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s

  # API service with canary deployment
  api:
    image: python:3.11-alpine
    networks:
      - app-network
      - backend-network
    environment:
      - VERSION=1.0.0
      - CANARY_PERCENTAGE=0
    command: |
      sh -c 'pip install --no-cache-dir flask &&
             cat > /app/app.py << "EOF"
from flask import Flask, jsonify, request
import os
import random

app = Flask(__name__)
version = os.getenv("VERSION", "1.0.0")
canary_pct = int(os.getenv("CANARY_PERCENTAGE", "0"))

@app.route("/health")
def health():
    return jsonify({
        "status": "healthy",
        "version": version,
        "canary": canary_pct > 0
    })

@app.route("/api/version")
def get_version():
    # Simulate canary behavior
    if canary_pct > 0 and random.randint(1, 100) <= canary_pct:
        return jsonify({
            "version": "2.0.0-canary",
            "deployment": "canary",
            "features": ["new-feature-1", "new-feature-2"]
        })
    
    return jsonify({
        "version": version,
        "deployment": "stable",
        "features": ["feature-1", "feature-2"]
    })

@app.route("/api/data")
def get_data():
    return jsonify({
        "data": list(range(10)),
        "version": version,
        "timestamp": "2023-01-01T00:00:00Z"
    })

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8080)
EOF
             python /app/app.py'
    deploy:
      replicas: 4
      update_config:
        parallelism: 2
        delay: 45s
        failure_action: rollback
        monitor: 180s
        max_failure_ratio: 0.25
        order: start-first
      rollback_config:
        parallelism: 2
        delay: 15s
        failure_action: pause
        monitor: 90s
      restart_policy:
        condition: on-failure
        delay: 15s
        max_attempts: 3
        window: 180s
      placement:
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          cpus: '1.0'
          memory: 256M
        reservations:
          cpus: '0.5'
          memory: 128M
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.api.rule=PathPrefix(`/api`)"
        - "traefik.http.services.api.loadbalancer.server.port=8080"
        - "traefik.http.services.api.loadbalancer.sticky=true"
        - "traefik.http.services.api.loadbalancer.healthcheck.path=/health"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/health')"]
      interval: 20s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Database with careful update strategy
  database:
    image: postgres:15-alpine
    networks:
      - backend-network
    environment:
      - POSTGRES_DB=appdb
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=secret123
    volumes:
      - db-data:/var/lib/postgresql/data
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 300s  # 5 minute delay for DB updates
        failure_action: pause
        monitor: 600s  # 10 minute monitoring
        max_failure_ratio: 0.0
        order: stop-first  # Stop old before starting new
      rollback_config:
        parallelism: 1
        delay: 60s
        failure_action: pause
        monitor: 300s
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
        window: 600s
      placement:
        constraints:
          - node.role == manager
          - node.labels.storage == ssd
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '1.0'
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d appdb"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # Worker service with rolling update
  worker:
    image: python:3.11-alpine
    networks:
      - backend-network
    environment:
      - WORKER_PROCESSES=2
      - QUEUE_NAME=default
    command: |
      sh -c 'pip install --no-cache-dir redis &&
             cat > /app/worker.py << "EOF"
import redis
import time
import json
import os
from datetime import datetime

r = redis.Redis(host="redis", port=6379, db=0)
processes = int(os.getenv("WORKER_PROCESSES", "1"))
queue = os.getenv("QUEUE_NAME", "default")

def process_job(job_data):
    print(f"Processing job: {job_data}")
    # Simulate work
    time.sleep(2)
    return {"status": "completed", "processed_at": datetime.now().isoformat()}

def worker_loop():
    while True:
        try:
            job = r.blpop(f"queue:{queue}", timeout=10)
            if job:
                job_data = json.loads(job[1])
                result = process_job(job_data)
                print(f"Job completed: {result}")
        except Exception as e:
            print(f"Error processing job: {e}")
            time.sleep(5)

if __name__ == "__main__":
    print(f"Starting worker for queue: {queue}")
    worker_loop()
EOF
             python /app/worker.py'
    deploy:
      replicas: 6
      update_config:
        parallelism: 2
        delay: 60s
        failure_action: rollback
        monitor: 120s
        max_failure_ratio: 0.33
        order: start-first
      rollback_config:
        parallelism: 3
        delay: 30s
        failure_action: pause
      restart_policy:
        condition: on-failure
        delay: 20s
        max_attempts: 5
        window: 300s
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          cpus: '1.0'
          memory: 256M
        reservations:
          cpus: '0.5'
          memory: 128M

  # Redis for queue management
  redis:
    image: redis:7-alpine
    networks:
      - backend-network
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 120s
        failure_action: rollback
        monitor: 300s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
      placement:
        constraints:
          - node.labels.cache == enabled
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3

  # Load balancer
  traefik:
    image: traefik:v3.0
    ports:
      - "80:80"
      - "8080:8080"
    networks:
      - app-network
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command:
      - --api.dashboard=true
      - --api.insecure=true
      - --providers.docker=true
      - --providers.docker.swarmmode=true
      - --providers.docker.exposedbydefault=false
      - --entrypoints.web.address=:80
      - --metrics.prometheus=true
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '1.0'
          memory: 256M
    healthcheck:
      test: ["CMD", "traefik", "healthcheck"]
      interval: 30s
      timeout: 5s
      retries: 3

networks:
  app-network:
    driver: overlay
    attachable: true
    
  backend-network:
    driver: overlay

volumes:
  db-data:
    driver: local
    
  redis-data:
    driver: local

configs:
  nginx-blue-config:
    external: true
    
  nginx-green-config:
    external: true