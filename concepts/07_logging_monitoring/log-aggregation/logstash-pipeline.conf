# 07_logging_monitoring/log-aggregation/logstash-pipeline.conf

input {
  # GELF input for Docker logs
  gelf {
    port => 12201
    type => "docker-gelf"
  }
  
  # Beats input for Filebeat/Metricbeat
  beats {
    port => 5044
    type => "beats"
  }
  
  # Syslog input
  syslog {
    port => 5000
    type => "syslog"
  }
  
  # HTTP input for webhook logs
  http {
    port => 8080
    type => "webhook"
  }
}

filter {
  # Parse Docker container logs
  if [type] == "docker-gelf" {
    # Add container metadata
    mutate {
      add_field => {
        "log_source" => "docker"
        "pipeline_version" => "1.0"
      }
      rename => {
        "host" => "docker_host"
        "tag" => "container_name"
      }
    }
    
    # Parse JSON logs if present
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "parsed_json"
      }
      
      # Move parsed fields to root level
      if [parsed_json] {
        ruby {
          code => "
            parsed = event.get('parsed_json')
            if parsed.is_a?(Hash)
              parsed.each { |k, v| event.set(k, v) }
            end
            event.remove('parsed_json')
          "
        }
      }
    }
    
    # Parse log level from message
    grok {
      match => { "message" => "%{WORD:log_level}" }
      tag_on_failure => ["_grok_log_level_failure"]
    }
    
    # Normalize log levels
    if [log_level] {
      mutate {
        uppercase => ["log_level"]
      }
      
      if [log_level] in ["DEBUG", "TRACE"] {
        mutate { replace => { "log_level" => "DEBUG" } }
      } else if [log_level] in ["WARN", "WARNING"] {
        mutate { replace => { "log_level" => "WARN" } }
      } else if [log_level] in ["ERR", "ERROR", "FATAL", "CRITICAL"] {
        mutate { replace => { "log_level" => "ERROR" } }
      } else if [log_level] not in ["INFO"] {
        mutate { replace => { "log_level" => "INFO" } }
      }
    }
  }
  
  # Parse Beats input
  if [type] == "beats" {
    # Handle Filebeat logs
    if [fields][logtype] == "application" {
      mutate {
        add_field => { "log_source" => "filebeat" }
      }
      
      # Parse application logs
      if [message] =~ /^\{.*\}$/ {
        json {
          source => "message"
        }
      } else {
        grok {
          match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} %{GREEDYDATA:log_message}" }
        }
      }
    }
    
    # Handle Metricbeat data
    if [fields][logtype] == "metrics" {
      mutate {
        add_field => { "log_source" => "metricbeat" }
      }
    }
  }
  
  # Parse syslog messages
  if [type] == "syslog" {
    mutate {
      add_field => { "log_source" => "syslog" }
    }
    
    # Additional syslog parsing
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
    }
  }
  
  # Common filters for all log types
  
  # Parse timestamps
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss" ]
      target => "@timestamp"
    }
  }
  
  # Add geolocation for IP addresses
  if [ip_address] {
    geoip {
      source => "ip_address"
      target => "geoip"
      add_tag => ["geoip"]
    }
  }
  
  # Parse user agents
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "ua"
    }
  }
  
  # Extract metrics from message
  if [message] =~ /response_time=(\d+)/ {
    grok {
      match => { "message" => "response_time=(?<response_time_ms>\d+)" }
    }
    mutate {
      convert => { "response_time_ms" => "integer" }
    }
  }
  
  # Add environment tags
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:development}"
      "cluster" => "${CLUSTER_NAME:local}"
      "processed_at" => "%{+yyyy-MM-dd HH:mm:ss}"
    }
  }
  
  # Remove empty or null fields
  ruby {
    code => "
      event.to_hash.each { |k, v|
        if v.nil? || (v.respond_to?(:empty?) && v.empty?)
          event.remove(k)
        end
      }
    "
  }
  
  # Security filtering - remove sensitive data
  mutate {
    remove_field => ["password", "token", "api_key", "secret", "authorization"]
  }
  
  # Add fingerprint for deduplication
  fingerprint {
    source => ["message", "container_name", "@timestamp"]
    target => "[@metadata][fingerprint]"
    method => "SHA1"
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logstash-%{+YYYY.MM.dd}"
    document_id => "%{[@metadata][fingerprint]}"
    template_name => "logstash"
    template => "/usr/share/logstash/config/elasticsearch-template.json"
    template_overwrite => true
  }
  
  # Output to file for debugging
  if [log_level] == "ERROR" {
    file {
      path => "/var/log/logstash/errors-%{+YYYY-MM-dd}.log"
      codec => json_lines
    }
  }
  
  # Output critical alerts to separate index
  if [log_level] == "ERROR" and [service] in ["payment", "authentication", "database"] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "critical-alerts-%{+YYYY.MM.dd}"
    }
  }
  
  # Debug output (can be disabled in production)
  if [@metadata][debug] == "true" {
    stdout {
      codec => rubydebug
    }
  }
}